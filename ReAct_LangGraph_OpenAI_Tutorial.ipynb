{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "607f3c21",
   "metadata": {},
   "source": [
    "\n",
    "# ReAct with LangGraph + OpenAI — End‑to‑End Tutorial\n",
    "\n",
    "This notebook implements the **ReAct** loop (Reason → Act → Observe → Repeat → Answer) using **LangGraph** for control flow and the **OpenAI API** for the language model with **tool calling**.\n",
    "\n",
    "### What you'll learn\n",
    "- How to encode the ReAct loop using **LangGraph**'s `StateGraph`\n",
    "- How to let an OpenAI model call **tools** (calculator + toy search) via function‑calling\n",
    "- How to build a **tool-execution node** and loop model→tools→model until a **final answer**\n",
    "- How to run **worked examples** (math, retrieval, multi‑hop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f497b07",
   "metadata": {},
   "source": [
    "\n",
    "> ⚙️ **Requirements**\n",
    ">\n",
    "> - Python ≥ 3.9  \n",
    "> - Packages: `openai`, `langgraph`, `pydantic` (install below)  \n",
    "> - An environment variable `OPENAI_API_KEY` set with a valid key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4c8040d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If needed, install the libraries (uncomment and run)\n",
    "# %pip install -U openai langgraph pydantic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aff3ad2",
   "metadata": {},
   "source": [
    "\n",
    "## 1) ReAct + LangGraph: how pieces fit\n",
    "\n",
    "- **Model Node**: calls the OpenAI chat model with tool schemas.  \n",
    "  The model can output:\n",
    "  - a normal message (finish, respond to user), or\n",
    "  - one or more **tool calls** (structured function calls).\n",
    "\n",
    "- **Tools Node**: executes each requested tool, returns **observations** as messages\n",
    "  back to the model.\n",
    "\n",
    "- **Control Flow**: LangGraph loops: `Model → (has tool calls?) → Tools → Model ... → Final Answer`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1def1078",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os, json, math, ast, operator, re\n",
    "from typing import TypedDict, List, Dict, Any\n",
    "\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Command\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d74d3652",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create OpenAI client\n",
    "# Make sure your environment has:  export OPENAI_API_KEY=sk-...\n",
    "#client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "MODEL_NAME = \"gpt-4o-mini\"  # choose a tool-capable model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfc68cd",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Define Tools\n",
    "\n",
    "We provide two simple tools:\n",
    "1. **calculator(query: str)** — safe arithmetic using a constrained AST evaluator\n",
    "2. **search(query: str)** — keyword search over a tiny in‑memory KB\n",
    "\n",
    "We'll expose them to the OpenAI model via **function (tool) schemas** so it can call them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81ee04d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Calculator ---\n",
    "def _safe_eval(expr: str) -> str:\n",
    "    node = ast.parse(expr, mode=\"eval\")\n",
    "    allowed_ops = {\n",
    "        ast.Add: operator.add, ast.Sub: operator.sub,\n",
    "        ast.Mult: operator.mul, ast.Div: operator.truediv,\n",
    "        ast.Pow: operator.pow, ast.Mod: operator.mod,\n",
    "        ast.FloorDiv: operator.floordiv, ast.USub: operator.neg\n",
    "    }\n",
    "    def eval_node(n):\n",
    "        if isinstance(n, ast.Expression):\n",
    "            return eval_node(n.body)\n",
    "        if hasattr(ast, \"Constant\") and isinstance(n, ast.Constant):\n",
    "            if isinstance(n.value, (int, float)):\n",
    "                return n.value\n",
    "            raise ValueError(\"Only numeric constants allowed.\")\n",
    "        if isinstance(n, ast.Num):\n",
    "            return n.n\n",
    "        if isinstance(n, ast.BinOp):\n",
    "            op = allowed_ops.get(type(n.op))\n",
    "            if not op: raise ValueError(\"Operator not allowed.\")\n",
    "            return op(eval_node(n.left), eval_node(n.right))\n",
    "        if isinstance(n, ast.UnaryOp) and type(n.op) in (ast.USub,):\n",
    "            return allowed_ops[type(n.op)](eval_node(n.operand))\n",
    "        raise ValueError(\"Expression not allowed.\")\n",
    "    return str(eval_node(node))\n",
    "\n",
    "def calculator(query: str) -> str:\n",
    "    try:\n",
    "        # normalize `x` to `*`, keep only math tokens\n",
    "        expr = re.sub(r'[^0-9\\+\\-\\*/\\.\\(\\)\\s]', '', query.replace('x','*'))\n",
    "        return _safe_eval(expr)\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd39253a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Tiny Search KB ---\n",
    "KB = {\n",
    "    \"capital_of_canada\": \"Ottawa is the capital of Canada.\",\n",
    "    \"ottawa_population\": \"Ottawa metro population is ~1,000,000 (approx.).\",\n",
    "    \"react_definition\": \"ReAct combines reasoning traces with tool use in a loop (Yao et al., 2022).\"\n",
    "}\n",
    "\n",
    "def search(query: str) -> str:\n",
    "    q = query.lower().strip()\n",
    "    if not q:\n",
    "        return \"ERROR: empty query\"\n",
    "    hits = []\n",
    "    for k, v in KB.items():\n",
    "        text = (k + \" \" + v).lower()\n",
    "        if all(tok in text for tok in q.split()):\n",
    "            hits.append(f\"- {k}: {v}\")\n",
    "    return \"Results:\\n\" + \"\\n\".join(hits) if hits else \"No results.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01562967",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Tool Schemas (function-calling)\n",
    "\n",
    "We describe each tool (name, parameters) so the model can call it when helpful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "687264a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# OpenAI \"tools\" parameter expects a specific JSON schema for functions\n",
    "TOOLS_SCHEMA = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"calculator\",\n",
    "            \"description\": \"Evaluate basic arithmetic expressions, e.g., '23 * 47'.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\"type\": \"string\", \"description\": \"The math expression to evaluate.\"}\n",
    "                },\n",
    "                \"required\": [\"query\"]\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"search\",\n",
    "            \"description\": \"Keyword search on a tiny in-memory knowledge base.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\"type\": \"string\", \"description\": \"Search string.\"}\n",
    "                },\n",
    "                \"required\": [\"query\"]\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b764fefd",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Graph State and utilities\n",
    "\n",
    "We keep a **chat transcript** as our state. The model may return **tool calls**; then we execute tools and append **observations** back as messages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7a0df94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: List[Dict[str, Any]]  # OpenAI-style messages: {role, content, tool_call_id?, name?}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eb3272f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def has_tool_calls(openai_message: Dict[str, Any]) -> bool:\n",
    "    tool_calls = openai_message.get(\"tool_calls\") or []\n",
    "    return len(tool_calls) > 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d254c788",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Node: Model call\n",
    "\n",
    "This node sends the running transcript to the OpenAI model with `tools=TOOLS_SCHEMA`.  \n",
    "The model either replies with a **final message** or with one or more **tool calls**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4718b324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(state: AgentState) -> AgentState:\n",
    "    messages = state.get(\"messages\", [])\n",
    "    if not messages or messages[0].get(\"role\") != \"system\":\n",
    "        messages = [{\"role\": \"system\", \"content\":\n",
    "            \"You are a helpful ReAct agent. Call tools when needed; keep chain-of-thought implicit; be concise.\"\n",
    "        }] + messages\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "        tools=TOOLS_SCHEMA,\n",
    "        tool_choice=\"auto\",\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    m = resp.choices[0].message\n",
    "\n",
    "    # Append as a minimal dict (role/content/tool_calls) to keep a consistent shape\n",
    "    messages.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": m.content,\n",
    "        \"tool_calls\": [\n",
    "            {\n",
    "                \"id\": tc.id,\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": tc.function.name,\n",
    "                    \"arguments\": tc.function.arguments,\n",
    "                },\n",
    "            } for tc in (m.tool_calls or [])\n",
    "        ],\n",
    "    })\n",
    "    return {\"messages\": messages}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e2cd23",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Node: Tool execution\n",
    "\n",
    "- Read pending tool calls from the last model message\n",
    "- Execute each tool in Python\n",
    "- Append **tool results** as messages of role `\"tool\"` with matching `tool_call_id`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5572a836",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def call_tools(state: AgentState) -> AgentState:\n",
    "    messages = state.get(\"messages\", [])\n",
    "    if not messages:\n",
    "        return {\"messages\": messages}\n",
    "    last = messages[-1]\n",
    "\n",
    "    tool_calls = last.get(\"tool_calls\") or []\n",
    "    for tc in tool_calls:\n",
    "        fn_name = tc.function.name\n",
    "        args = {}\n",
    "        try:\n",
    "            args = json.loads(tc.function.arguments or \"{}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Dispatch to our Python tools\n",
    "        if fn_name == \"calculator\":\n",
    "            result = calculator(**args)\n",
    "        elif fn_name == \"search\":\n",
    "            result = search(**args)\n",
    "        else:\n",
    "            result = f\"ERROR: Unknown tool '{fn_name}'\"\n",
    "\n",
    "        # Add the observation so the model can read it\n",
    "        messages.append({\n",
    "            \"role\": \"tool\",\n",
    "            \"tool_call_id\": tc.id,\n",
    "            \"name\": fn_name,\n",
    "            \"content\": result,\n",
    "        })\n",
    "    return {\"messages\": messages}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43696ef3",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Build the LangGraph\n",
    "\n",
    "- `START → model`\n",
    "- If the model returns **tool calls**, go to `tools` then back to `model`\n",
    "- Else, stop at `END` (we have a final answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3d45bd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Helper to normalize OpenAI tool_call shapes (dict or object) ---\n",
    "def _tc_fields(tc):\n",
    "    \"\"\"\n",
    "    Returns (tool_call_id, name, arguments_json_str) for either:\n",
    "    - dict-shaped tool call: {\"id\": \"...\", \"function\": {\"name\": \"...\", \"arguments\": \"...\"}}\n",
    "    - object-shaped tool call: tc.id, tc.function.name, tc.function.arguments\n",
    "    \"\"\"\n",
    "    if isinstance(tc, dict):\n",
    "        fn = tc.get(\"function\") or {}\n",
    "        return tc.get(\"id\"), fn.get(\"name\"), fn.get(\"arguments\")\n",
    "    # object-like (SDK types)\n",
    "    return getattr(tc, \"id\", None), getattr(tc.function, \"name\", None), getattr(tc.function, \"arguments\", None)\n",
    "\n",
    "def call_tools(state: AgentState) -> AgentState:\n",
    "    messages = state.get(\"messages\", [])\n",
    "    if not messages:\n",
    "        return {\"messages\": messages}\n",
    "\n",
    "    last = messages[-1]\n",
    "    tool_calls = last.get(\"tool_calls\") or []\n",
    "\n",
    "    for tc in tool_calls:\n",
    "        tc_id, fn_name, args_json = _tc_fields(tc)\n",
    "        try:\n",
    "            args = json.loads(args_json or \"{}\")\n",
    "        except Exception:\n",
    "            args = {}\n",
    "\n",
    "        # Dispatch to our Python tools\n",
    "        if fn_name == \"calculator\":\n",
    "            result = calculator(**args)\n",
    "        elif fn_name == \"search\":\n",
    "            result = search(**args)\n",
    "        else:\n",
    "            result = f\"ERROR: Unknown tool '{fn_name}'\"\n",
    "\n",
    "        # Append observation for the model to read\n",
    "        messages.append({\n",
    "            \"role\": \"tool\",\n",
    "            \"tool_call_id\": tc_id,\n",
    "            \"name\": fn_name,\n",
    "            \"content\": result,\n",
    "        })\n",
    "\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "\n",
    "def should_continue(state: AgentState) -> str:\n",
    "    # If last assistant message contains tool calls, run tools; else, we are done.\n",
    "    messages = state.get(\"messages\", [])\n",
    "    if not messages:\n",
    "        return \"end\"\n",
    "    last = messages[-1]\n",
    "    if last.get(\"role\") == \"assistant\" and has_tool_calls(last):\n",
    "        return \"tools\"\n",
    "    return \"end\"\n",
    "\n",
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"model\", call_model)\n",
    "graph.add_node(\"tools\", call_tools)\n",
    "\n",
    "graph.add_edge(START, \"model\")\n",
    "graph.add_conditional_edges(\n",
    "    \"model\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"tools\": \"tools\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "graph.add_edge(\"tools\", \"model\")\n",
    "\n",
    "app = graph.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f16463",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Helper runner\n",
    "\n",
    "`run_react(query)` seeds the chat with the user message, runs the graph, and prints the transcript.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7b75eea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_react(query: str, max_steps: int = 8) -> List[Dict[str, Any]]:\n",
    "    state: AgentState = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ]\n",
    "    }\n",
    "    # Each `app.invoke` will advance until END, but we rely on internal loop.\n",
    "    # `max_steps` serves as a guard if the model/tool loop fails to converge.\n",
    "    for _ in range(max_steps):\n",
    "        state = app.invoke(state)\n",
    "        # After invoke, if the last message is from assistant with no tool_calls, we're done\n",
    "        last = state[\"messages\"][-1]\n",
    "        if last[\"role\"] == \"assistant\" and not has_tool_calls(last):\n",
    "            break\n",
    "    return state[\"messages\"]\n",
    "\n",
    "def print_transcript(messages: List[Dict[str, Any]]):\n",
    "    for m in messages:\n",
    "        role = m.get(\"role\")\n",
    "        if role == \"assistant\":\n",
    "            print(\"\\n[assistant]\")\n",
    "            if m.get(\"tool_calls\"):\n",
    "                print(\"(assistant requested tools)\")\n",
    "        elif role == \"tool\":\n",
    "            print(f\"\\n[tool:{m.get('name')}] → {m.get('content')}\")\n",
    "        elif role == \"user\":\n",
    "            print(f\"\\n[user] {m.get('content')}\")\n",
    "        elif role == \"system\":\n",
    "            pass  # omit for brevity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30bb390",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Examples\n",
    "\n",
    "> Note: These require a valid `OPENAI_API_KEY` and internet access.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1571b239",
   "metadata": {},
   "source": [
    "### 9.1 Math (calculator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "496c791b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1782643/1016456093.py:17: DeprecationWarning: ast.Num is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  if isinstance(n, ast.Num):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[user] What is 23 * 47? Please compute it.\n",
      "\n",
      "[assistant]\n",
      "(assistant requested tools)\n",
      "\n",
      "[tool:calculator] → 1081\n",
      "\n",
      "[assistant]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "msgs = run_react(\"What is 23 * 47? Please compute it.\")\n",
    "print_transcript(msgs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc595ff",
   "metadata": {},
   "source": [
    "### 9.2 Retrieval (search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c00744c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[user] What is ReAct in language models?\n",
      "\n",
      "[assistant]\n",
      "(assistant requested tools)\n",
      "\n",
      "[tool:search] → No results.\n",
      "\n",
      "[assistant]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "msgs = run_react(\"What is ReAct in language models?\")\n",
    "print_transcript(msgs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91ab82e",
   "metadata": {},
   "source": [
    "### 9.3 Multi‑hop (search → compute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5c558b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[user] What is the population of the capital of Canada?\n",
      "\n",
      "[assistant]\n",
      "(assistant requested tools)\n",
      "\n",
      "[tool:search] → No results.\n",
      "\n",
      "[assistant]\n",
      "(assistant requested tools)\n",
      "\n",
      "[tool:search] → No results.\n",
      "\n",
      "[assistant]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "msgs = run_react(\"What is the population of the capital of Canada?\")\n",
    "print_transcript(msgs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c54bfcd",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Extensions, Safety & Tips\n",
    "\n",
    "- **Guardrails**: validate tool inputs, cap steps with `max_steps`, and block risky code tools.\n",
    "- **Citations**: require the model to cite which tool results it used before final answers.\n",
    "- **Caching**: memoize tool outputs to avoid repeated calls.\n",
    "- **Memory**: store facts learned during the session and surface them to the model.\n",
    "- **UI**: plug this graph into Streamlit or FastAPI for a simple ReAct assistant.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
